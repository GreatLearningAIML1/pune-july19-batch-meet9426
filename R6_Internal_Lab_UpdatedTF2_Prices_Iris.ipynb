{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LeuZYeuTMeIn",
    "outputId": "4443b559-0f43-4c36-aeb3-7308f37bb1b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjtb-EMcm5K0"
   },
   "outputs": [],
   "source": [
    "#Enable Eager Execution if using tensflow version < 2.0\n",
    "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K"
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FhllFLyKOB6N",
    "outputId": "aeb3d44b-5bad-4f96-de7d-f9e9315ab467"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive/')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiObW4V4SIOz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/Parikshit/Desktop/great lakes course/Week 6 Assignment/Internal lab/prices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7K8pWsNQOB6X",
    "outputId": "911e8915-c1b1-4eb7-eb8a-18fd8b13429e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data.drop(columns=['date','symbol'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "3af0986e-1e48-4fdc-ddab-268bf2a1d7b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGnlxp5GOIE7"
   },
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "sbM4vflVOSAC",
    "outputId": "5b5ec3b4-150c-4741-b185-d15857ef8c7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.1636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.3864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.4895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.0063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.4086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>2.1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>1.9824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>37.1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>6.5686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>5.6043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      volume\n",
       "0     2.1636\n",
       "1     2.3864\n",
       "2     2.4895\n",
       "3     2.0063\n",
       "4     1.4086\n",
       "..       ...\n",
       "995   2.1332\n",
       "996   1.9824\n",
       "997  37.1528\n",
       "998   6.5686\n",
       "999   5.6043\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.iloc[:,4:]/1000000\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.20,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train =np.array(X_train).astype('float32')\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "Y_train =np.array(Y_train).astype('float32')\n",
    "Y_test = np.array(Y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "transformer = Normalizer()\n",
    "X_train = transformer.fit_transform(X_train)\n",
    "X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62"
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A"
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "W = tf.zeros(shape=(4, 1))\n",
    "b = tf.zeros(shape=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F"
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prediction(X_train, W, b):\n",
    "    y_pred = tf.add(tf.matmul(X_train, W), b)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M"
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss(Y_train, y_predicted):\n",
    "    diff = Y_train - y_predicted\n",
    "    sqr = tf.square(diff)\n",
    "    avg = tf.reduce_mean(sqr)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U"
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(X_train, Y_train, w, b, learning_rate=0.01):\n",
    "    \n",
    "    # Record mathematical operations on 'tape' to calculate loss\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch([w,b])\n",
    "        current_prediction = prediction(X_train, w, b)\n",
    "        current_loss = loss(Y_train, current_prediction)\n",
    "    \n",
    "    # Calculate Gradients for Loss with respect to Weights and Bias\n",
    "    dw, db = t.gradient(current_loss,[w, b])\n",
    "    \n",
    "    # Update Weights and Bias\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e"
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Train loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DVvgj7eQOB7f",
    "outputId": "24bda3f0-d957-478b-cce7-2dc740743267"
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMV launch failed:  m=4, n=800 [Op:MatMul] name: MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-537dddf4464d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Current Training Loss on iteration'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' -- '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-043231a6b933>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X_train, Y_train, w, b, learning_rate)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mcurrent_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-8408ff3a04dc>\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(X_train, W, b)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2763\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2765\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6124\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6126\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6127\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6128\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Blas GEMV launch failed:  m=4, n=800 [Op:MatMul] name: MatMul/"
     ]
    }
   ],
   "source": [
    "for i in range(100):    \n",
    "    W, b = train(X_train, Y_train, W, b)\n",
    "    print('Current Training Loss on iteration', i, ' -- ', loss(Y_train, prediction(X_train, W, b)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "_HWzEqLLWVHb",
    "outputId": "e4bf8429-0348-414c-c527-ea5ba84da64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Training Loss on iteration :  0  --  172.03566\n",
      "Current Training Loss on iteration :  5  --  172.03319\n",
      "Current Training Loss on iteration :  10  --  172.03151\n",
      "Current Training Loss on iteration :  15  --  172.0304\n",
      "Current Training Loss on iteration :  20  --  172.02963\n",
      "Current Training Loss on iteration :  25  --  172.0291\n",
      "Current Training Loss on iteration :  30  --  172.02875\n",
      "Current Training Loss on iteration :  35  --  172.0285\n",
      "Current Training Loss on iteration :  40  --  172.02832\n",
      "Current Training Loss on iteration :  45  --  172.02818\n",
      "Current Training Loss on iteration :  50  --  172.02809\n",
      "Current Training Loss on iteration :  55  --  172.02803\n",
      "Current Training Loss on iteration :  60  --  172.02792\n",
      "Current Training Loss on iteration :  65  --  172.02791\n",
      "Current Training Loss on iteration :  70  --  172.02785\n",
      "Current Training Loss on iteration :  75  --  172.02779\n",
      "Current Training Loss on iteration :  80  --  172.02777\n",
      "Current Training Loss on iteration :  85  --  172.02771\n",
      "Current Training Loss on iteration :  90  --  172.02765\n",
      "Current Training Loss on iteration :  95  --  172.02763\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):    \n",
    "    W, b = train(X_train, Y_train, W, b)\n",
    "    if i%5 == 0:\n",
    "      print('Current Training Loss on iteration : ', i,' -- ', loss(Y_train, prediction(X_train, W, b)).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r",
    "outputId": "0be4d823-dd01-485d-848c-6c59abba7484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of W :  tf.Tensor(\n",
      "[[1.2862535]\n",
      " [1.3554296]\n",
      " [1.2863495]\n",
      " [1.3583084]], shape=(4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print ('shape of W : ', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vhDtOv5UOB7x",
    "outputId": "4cfde09c-1a5d-442c-e517-b4abe5409a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of b :  tf.Tensor([2.6439803], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print ('shape of b : ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gKGvUWahcihp",
    "outputId": "9008be0c-ae93-4a4d-faf9-718eadbf6578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=16593, shape=(200, 1), dtype=float32, numpy=\n",
       "array([[5.287775 ],\n",
       "       [5.2875805],\n",
       "       [5.287441 ],\n",
       "       [5.2872868],\n",
       "       [5.2873816],\n",
       "       [5.2880764],\n",
       "       [5.287963 ],\n",
       "       [5.2875824],\n",
       "       [5.287302 ],\n",
       "       [5.2872744],\n",
       "       [5.287551 ],\n",
       "       [5.2878036],\n",
       "       [5.2879677],\n",
       "       [5.2871723],\n",
       "       [5.2877884],\n",
       "       [5.287678 ],\n",
       "       [5.2878547],\n",
       "       [5.28751  ],\n",
       "       [5.287779 ],\n",
       "       [5.287405 ],\n",
       "       [5.287567 ],\n",
       "       [5.2871876],\n",
       "       [5.287155 ],\n",
       "       [5.28728  ],\n",
       "       [5.287411 ],\n",
       "       [5.287141 ],\n",
       "       [5.287696 ],\n",
       "       [5.28675  ],\n",
       "       [5.286813 ],\n",
       "       [5.2871933],\n",
       "       [5.2879047],\n",
       "       [5.287507 ],\n",
       "       [5.2879295],\n",
       "       [5.2877893],\n",
       "       [5.287151 ],\n",
       "       [5.2876463],\n",
       "       [5.287258 ],\n",
       "       [5.287209 ],\n",
       "       [5.287731 ],\n",
       "       [5.2872295],\n",
       "       [5.287771 ],\n",
       "       [5.2864695],\n",
       "       [5.287985 ],\n",
       "       [5.287569 ],\n",
       "       [5.2872295],\n",
       "       [5.2872   ],\n",
       "       [5.287592 ],\n",
       "       [5.287517 ],\n",
       "       [5.287467 ],\n",
       "       [5.2875714],\n",
       "       [5.287466 ],\n",
       "       [5.2873163],\n",
       "       [5.287298 ],\n",
       "       [5.287319 ],\n",
       "       [5.2872267],\n",
       "       [5.287299 ],\n",
       "       [5.2873306],\n",
       "       [5.287521 ],\n",
       "       [5.2874813],\n",
       "       [5.2872696],\n",
       "       [5.2878036],\n",
       "       [5.2874284],\n",
       "       [5.2873425],\n",
       "       [5.2875633],\n",
       "       [5.2871923],\n",
       "       [5.287214 ],\n",
       "       [5.287423 ],\n",
       "       [5.2874784],\n",
       "       [5.2877727],\n",
       "       [5.287359 ],\n",
       "       [5.2877426],\n",
       "       [5.287158 ],\n",
       "       [5.287668 ],\n",
       "       [5.287873 ],\n",
       "       [5.287424 ],\n",
       "       [5.2872148],\n",
       "       [5.287467 ],\n",
       "       [5.287062 ],\n",
       "       [5.287263 ],\n",
       "       [5.287632 ],\n",
       "       [5.28708  ],\n",
       "       [5.2870502],\n",
       "       [5.287378 ],\n",
       "       [5.287222 ],\n",
       "       [5.2874584],\n",
       "       [5.2871943],\n",
       "       [5.287369 ],\n",
       "       [5.2875595],\n",
       "       [5.287401 ],\n",
       "       [5.287273 ],\n",
       "       [5.28738  ],\n",
       "       [5.287305 ],\n",
       "       [5.287552 ],\n",
       "       [5.2873287],\n",
       "       [5.287401 ],\n",
       "       [5.2879615],\n",
       "       [5.2871246],\n",
       "       [5.287567 ],\n",
       "       [5.287141 ],\n",
       "       [5.287525 ],\n",
       "       [5.2877755],\n",
       "       [5.2874103],\n",
       "       [5.2872014],\n",
       "       [5.28749  ],\n",
       "       [5.2875686],\n",
       "       [5.287385 ],\n",
       "       [5.2874784],\n",
       "       [5.287587 ],\n",
       "       [5.2874966],\n",
       "       [5.2874217],\n",
       "       [5.2876124],\n",
       "       [5.287323 ],\n",
       "       [5.287218 ],\n",
       "       [5.2873483],\n",
       "       [5.287815 ],\n",
       "       [5.287395 ],\n",
       "       [5.2874107],\n",
       "       [5.287758 ],\n",
       "       [5.287467 ],\n",
       "       [5.2871943],\n",
       "       [5.2871943],\n",
       "       [5.287325 ],\n",
       "       [5.287299 ],\n",
       "       [5.2872567],\n",
       "       [5.287203 ],\n",
       "       [5.2871933],\n",
       "       [5.2874794],\n",
       "       [5.287475 ],\n",
       "       [5.2871895],\n",
       "       [5.2873096],\n",
       "       [5.287228 ],\n",
       "       [5.287285 ],\n",
       "       [5.2875233],\n",
       "       [5.2871695],\n",
       "       [5.2872114],\n",
       "       [5.2872443],\n",
       "       [5.2874126],\n",
       "       [5.287277 ],\n",
       "       [5.2878423],\n",
       "       [5.2873516],\n",
       "       [5.2871304],\n",
       "       [5.2875853],\n",
       "       [5.2873573],\n",
       "       [5.2874203],\n",
       "       [5.2872677],\n",
       "       [5.28741  ],\n",
       "       [5.287588 ],\n",
       "       [5.2878075],\n",
       "       [5.2874894],\n",
       "       [5.2878075],\n",
       "       [5.287743 ],\n",
       "       [5.287372 ],\n",
       "       [5.2876234],\n",
       "       [5.2876177],\n",
       "       [5.2872725],\n",
       "       [5.287529 ],\n",
       "       [5.2872295],\n",
       "       [5.2871857],\n",
       "       [5.2872515],\n",
       "       [5.28743  ],\n",
       "       [5.287851 ],\n",
       "       [5.287221 ],\n",
       "       [5.287652 ],\n",
       "       [5.287445 ],\n",
       "       [5.287279 ],\n",
       "       [5.287774 ],\n",
       "       [5.287792 ],\n",
       "       [5.287404 ],\n",
       "       [5.28716  ],\n",
       "       [5.2872376],\n",
       "       [5.287477 ],\n",
       "       [5.2872295],\n",
       "       [5.2873664],\n",
       "       [5.2872076],\n",
       "       [5.2879896],\n",
       "       [5.287532 ],\n",
       "       [5.287198 ],\n",
       "       [5.287747 ],\n",
       "       [5.287714 ],\n",
       "       [5.287249 ],\n",
       "       [5.2877703],\n",
       "       [5.287365 ],\n",
       "       [5.287161 ],\n",
       "       [5.287579 ],\n",
       "       [5.2874784],\n",
       "       [5.287158 ],\n",
       "       [5.2873707],\n",
       "       [5.287738 ],\n",
       "       [5.2874584],\n",
       "       [5.28652  ],\n",
       "       [5.2871914],\n",
       "       [5.287309 ],\n",
       "       [5.2879057],\n",
       "       [5.2876215],\n",
       "       [5.287175 ],\n",
       "       [5.287484 ],\n",
       "       [5.287241 ],\n",
       "       [5.2873707],\n",
       "       [5.287263 ],\n",
       "       [5.2875786]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=prediction(X_test,W,b)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"C:/Users/Parikshit/Desktop/great lakes course/Week 6 Assignment/Internal lab/11_Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMiO2z6tehNt"
   },
   "outputs": [],
   "source": [
    "# import labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()\n",
    "# apply le on categorical feature columns\n",
    "iris.Species = le.fit_transform(iris.Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJr5dYnocihm"
   },
   "outputs": [],
   "source": [
    "y_iris = tf.keras.utils.to_categorical(iris.Species, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RyMQoLMucihj",
    "outputId": "e3410d37-fce7-4306-d025-199f7e8da2ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_iris = iris.iloc[:,1:5]\n",
    "#y_iris = iris.Species\n",
    "y_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9SVhV3Pf42w"
   },
   "outputs": [],
   "source": [
    "# Initialize Sequential model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(4, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfYQ2mvggA6E"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create optimizer with non-default learning rate\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7p5uvrlygLBw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(X_iris, y_iris, test_size=0.20,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4qLEdHPscihx",
    "outputId": "3e44cbfb-89a5-4ee9-bae0-2a93d9343485"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1202 19:50:14.527455  3120 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 0s 2ms/sample - loss: 1.0969 - accuracy: 0.3333 - val_loss: 1.0682 - val_accuracy: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 270us/sample - loss: 1.0409 - accuracy: 0.3333 - val_loss: 0.9929 - val_accuracy: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 207us/sample - loss: 0.9829 - accuracy: 0.3583 - val_loss: 0.9743 - val_accuracy: 0.6667\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 202us/sample - loss: 0.9458 - accuracy: 0.5583 - val_loss: 0.9194 - val_accuracy: 0.6333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 255us/sample - loss: 0.9179 - accuracy: 0.6000 - val_loss: 0.9013 - val_accuracy: 0.6333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 287us/sample - loss: 0.9049 - accuracy: 0.6083 - val_loss: 0.8813 - val_accuracy: 0.6667\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 288us/sample - loss: 0.8933 - accuracy: 0.6167 - val_loss: 0.8677 - val_accuracy: 0.6667\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 261us/sample - loss: 0.8676 - accuracy: 0.6667 - val_loss: 0.8552 - val_accuracy: 0.6667\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 264us/sample - loss: 0.8566 - accuracy: 0.6667 - val_loss: 0.8429 - val_accuracy: 0.6667\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.8456 - accuracy: 0.6667 - val_loss: 0.8315 - val_accuracy: 0.6667\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 273us/sample - loss: 0.8330 - accuracy: 0.6667 - val_loss: 0.8239 - val_accuracy: 0.6667\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 238us/sample - loss: 0.8286 - accuracy: 0.6667 - val_loss: 0.8103 - val_accuracy: 0.6667\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 236us/sample - loss: 0.8162 - accuracy: 0.6667 - val_loss: 0.8027 - val_accuracy: 0.6667\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 272us/sample - loss: 0.8036 - accuracy: 0.6667 - val_loss: 0.7909 - val_accuracy: 0.6667\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.7929 - accuracy: 0.6667 - val_loss: 0.7803 - val_accuracy: 0.6667\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 217us/sample - loss: 0.7857 - accuracy: 0.6667 - val_loss: 0.7712 - val_accuracy: 0.6667\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 246us/sample - loss: 0.7763 - accuracy: 0.6667 - val_loss: 0.7640 - val_accuracy: 0.6667\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 282us/sample - loss: 0.7660 - accuracy: 0.6667 - val_loss: 0.7548 - val_accuracy: 0.6667\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 252us/sample - loss: 0.7575 - accuracy: 0.6667 - val_loss: 0.7474 - val_accuracy: 0.6667\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 258us/sample - loss: 0.7496 - accuracy: 0.6667 - val_loss: 0.7382 - val_accuracy: 0.6667\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 248us/sample - loss: 0.7425 - accuracy: 0.6667 - val_loss: 0.7305 - val_accuracy: 0.6667\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 260us/sample - loss: 0.7350 - accuracy: 0.6667 - val_loss: 0.7234 - val_accuracy: 0.6667\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 229us/sample - loss: 0.7304 - accuracy: 0.6667 - val_loss: 0.7203 - val_accuracy: 0.6667\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 247us/sample - loss: 0.7213 - accuracy: 0.6750 - val_loss: 0.7101 - val_accuracy: 0.6667\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 207us/sample - loss: 0.7148 - accuracy: 0.6667 - val_loss: 0.7032 - val_accuracy: 0.6667\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 216us/sample - loss: 0.7084 - accuracy: 0.6667 - val_loss: 0.6970 - val_accuracy: 0.6667\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 203us/sample - loss: 0.7011 - accuracy: 0.6667 - val_loss: 0.6913 - val_accuracy: 0.6667\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 226us/sample - loss: 0.6950 - accuracy: 0.6667 - val_loss: 0.6850 - val_accuracy: 0.6667\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 197us/sample - loss: 0.6915 - accuracy: 0.6750 - val_loss: 0.6792 - val_accuracy: 0.6667\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 230us/sample - loss: 0.6840 - accuracy: 0.6750 - val_loss: 0.6747 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23ed3e1b2e8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-SgSSdRcih5"
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558
    },
    "colab_type": "code",
    "id": "GBgKZkhkcih6",
    "outputId": "ade4650b-04e6-43eb-b23f-eac3e3bfca13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1202 19:50:16.432182  3120 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.00362541, 0.30818567, 0.6881889 ],\n",
       "       [0.06420951, 0.42890084, 0.50688964],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.01516112, 0.37305996, 0.611779  ],\n",
       "       [0.05910607, 0.42685047, 0.51404345],\n",
       "       [0.07047994, 0.43091762, 0.49860248],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.07070117, 0.43141055, 0.4978883 ],\n",
       "       [0.05990533, 0.42717877, 0.51291597],\n",
       "       [0.01327026, 0.36621106, 0.6205187 ],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.02828279, 0.4007393 , 0.57097787],\n",
       "       [0.00906469, 0.34946483, 0.64147043],\n",
       "       [0.02006944, 0.3851078 , 0.59482276],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.03044245, 0.40306965, 0.56648797],\n",
       "       [0.01599538, 0.37518153, 0.60882306],\n",
       "       [0.02243279, 0.38974613, 0.5878211 ],\n",
       "       [0.0374707 , 0.41204053, 0.55048877],\n",
       "       [0.5057226 , 0.3004017 , 0.19387567],\n",
       "       [0.01000526, 0.353206  , 0.6367887 ],\n",
       "       [0.0156656 , 0.37519124, 0.60914314],\n",
       "       [0.07227521, 0.43215242, 0.4955723 ],\n",
       "       [0.11968474, 0.43677393, 0.44354132],\n",
       "       [0.04354107, 0.41711476, 0.5393442 ]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# how to load model\\njson_file = open(\\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\nloaded_model = model_from_json(loaded_model_json)\\n# load weights into new model\\nloaded_model.load_weights(\"model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pickle\n",
    "#filename = 'finalized_model.sav'\n",
    "#pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\"\"\"\n",
    "# how to load model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# How to load model from yaml\\n# load YAML and create model\\nyaml_file = open(\\'model.yaml\\', \\'r\\')\\nloaded_model_yaml = yaml_file.read()\\nyaml_file.close()\\nloaded_model = model_from_yaml(loaded_model_yaml)\\n# load weights into new model\\nloaded_model.load_weights(\"model.h5\")\\nprint(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\"\"\"\n",
    "# How to load model from yaml\n",
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# load model\\nmodel = load_model('model.h5')\\n# summarize model.\\nmodel.summary()\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model in keras\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# how to load model from keras \n",
    "\"\"\"\n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# summarize model.\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e0dcef2da667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Initialize Sequential model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize Sequential model\n",
    "model_layer = tf.keras.models.Sequential()\n",
    "model_layer.add(tf.keras.layers.Dense(4, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "model_layer.add(tf.keras.layers.Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "model_layer.add(tf.keras.layers.Dense(3, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "model_layer.add(tf.keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xSypb_u8OB7e",
    "DOL2ncA1OB7q"
   ],
   "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
